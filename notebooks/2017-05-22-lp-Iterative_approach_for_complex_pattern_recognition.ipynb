{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this notebook i am going to expose the ability of an iterative approach to recognize complex patterns aginst conventional feedfoward approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import floor\n",
    "from rnn_cell import *\n",
    "\n",
    "class IterativeCell(object):\n",
    "\n",
    "    def __init__(self, size, max_iterations=50., initial_iterate_prob=0.5,\n",
    "                 iterate_prob_decay=0.5, allow_cell_reactivation=True, add_summaries=False, device_to_run_at=None):\n",
    "        self._device_to_run_at = device_to_run_at\n",
    "        self._sixe = size\n",
    "\n",
    "        if max_iterations < 1:\n",
    "            raise \"The maximum amount of iterations to perform must be a natural value\"\n",
    "\n",
    "        if initial_iterate_prob <= 0 or initial_iterate_prob >= 1:\n",
    "            raise \"iteration_prob must be a value between 0 and 1\"\n",
    "\n",
    "        self._max_iteration_constant = max_iterations\n",
    "        self._initial_iterate_prob_constant = initial_iterate_prob\n",
    "        self._iterate_prob_decay_constant = iterate_prob_decay\n",
    "        self._allow_reactivation = allow_cell_reactivation\n",
    "        self._should_add_summaries = add_summaries\n",
    "        self._already_added_summaries = []\n",
    "\n",
    "    @property\n",
    "    def input_size(self):\n",
    "        return self._internal_nn._input_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._internal_nn.output_size\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._internal_nn.state_size\n",
    "\n",
    "    def __call__(self, input, scope=None):\n",
    "        if self._should_add_summaries:\n",
    "            self.add_pre_execution_summaries(input)\n",
    "\n",
    "        with vs.variable_scope(scope or type(self).__name__):\n",
    "            loop_vars = [input, tf.zeros([self.output_size]), tf.constant(0.0),\n",
    "                         tf.constant(self._max_iteration_constant), tf.constant(self._initial_iterate_prob_constant),\n",
    "                         tf.constant(self._iterate_prob_decay_constant), tf.ones(tf.shape(input)), tf.zeros([input.get_shape().dims[0].value, self.output_size]),\n",
    "                         tf.constant(True)]\n",
    "            loop_vars[0], loop_vars[1], loop_vars[2], loop_vars[3], loop_vars[4], loop_vars[5], loop_vars[6], loop_vars[\n",
    "                7], loop_vars[8], loop_vars[9], loop_vars[10] = tf.while_loop(iterativeLSTM_LoopCondition, iterativeLSTM_Iteration, loop_vars)\n",
    "\n",
    "        #_, loop_vars[0] = array_ops.split(1, 2, loop_vars[1])\n",
    "\n",
    "        if self._should_add_summaries:\n",
    "            self.add_post_execution_summaries(input, state, loop_vars[0] , loop_vars[1], loop_vars[4], None, loop_vars[9], None)\n",
    "\n",
    "        return loop_vars[0]\n",
    "\n",
    "    def add_pre_execution_summaries(self, input, state):\n",
    "        if not self._already_added_summaries.__contains__(tf.get_variable_scope().name +\n",
    "                                                                  \"/pre_execution_input_entropy\"):\n",
    "            variable_summaries(calculate_feature_entropy(input),\n",
    "                               tf.get_variable_scope().name + \"/pre_execution_input_entropy\", add_histogram=False)\n",
    "            self._already_added_summaries.append(tf.get_variable_scope().name + \"/pre_execution_input_entropy\")\n",
    "\n",
    "    def add_post_execution_summaries(self, initial_input, initial_state, final_output, final_state, number_of_iterations_performed,\n",
    "                                     final_iterate_prob, final_iterations_counts, final_iteration_activations):\n",
    "        if not self._already_added_summaries.__contains__(tf.get_variable_scope().name+\"/iterations_performed\"):\n",
    "            variable_summaries(number_of_iterations_performed, tf.get_variable_scope().name+\"/iterations_performed\")\n",
    "            self._already_added_summaries.append(tf.get_variable_scope().name+\"/iterations_performed\")\n",
    "\n",
    "            self._already_added_summaries.append(tf.get_variable_scope().name+\"/iterations_counts\")\n",
    "            variable_summaries(final_iterations_counts, tf.get_variable_scope().name+\"/iterations_counts\")\n",
    "\n",
    "            variable_summaries(calculate_feature_entropy(final_output),\n",
    "                               tf.get_variable_scope().name + \"/post_execution_output_entropy\", add_histogram=False)\n",
    "            self._already_added_summaries.append(tf.get_variable_scope().name + \"/post_execution_output_entropy\")\n",
    "\n",
    "            variable_summaries(calculate_feature_vectors_kl_divergence(initial_input, final_output),\n",
    "                               tf.get_variable_scope().name + \"/improved_from_former_kl_divergence\", add_histogram=False)\n",
    "            self._already_added_summaries.append(tf.get_variable_scope().name + \"/improved_from_former_kl_divergence\")\n",
    "\n",
    "            variable_summaries(calculate_feature_vectors_kl_divergence(final_output, initial_input),\n",
    "                               tf.get_variable_scope().name + \"/former_from_improved_kl_divergence\", add_histogram=False)\n",
    "            self._already_added_summaries.append(tf.get_variable_scope().name + \"/former_from_improved_kl_divergence\")\n",
    "\n",
    "def calculate_feature_entropy(feature_vector):\n",
    "    return - feature_vector * tf.log(feature_vector) - (1 - feature_vector) * tf.log(1 - feature_vector)\n",
    "\n",
    "def calculate_feature_vectors_kl_divergence(former_feature_vector, updated_feature_vector):\n",
    "    return former_feature_vector * (tf.log(former_feature_vector) - tf.log(updated_feature_vector)) + (1 - former_feature_vector) * (tf.log(1 - former_feature_vector) - tf.log(1 - updated_feature_vector))\n",
    "\n",
    "\n",
    "def iterativeLSTM_Iteration(inputs, num_units, iteration_number, max_iterations,\n",
    "                            iteration_prob, iteration_prob_decay, iteration_activation, iteration_count, \n",
    "                            keep_looping):\n",
    "\n",
    "    output, new_iteration_activation = iterativeLSTM(inputs, state, num_units.get_shape().dims[0].value,\n",
    "                                                                                        forget_bias, iteration_activation,\n",
    "                                                                                        iteration_count, iteration_prob)\n",
    "    iteration_flag = tf.reduce_max(new_iteration_activation)\n",
    "\n",
    "    iteration_count = iteration_count + iteration_flag\n",
    "\n",
    "    new_iteration_number = iteration_number + iteration_flag\n",
    "\n",
    "    do_keep_looping = tf.logical_and(tf.less(new_iteration_number, max_iterations), tf.equal(iteration_flag, tf.constant(1.0)))\n",
    "\n",
    "    new_iteration_prob = iteration_prob * iteration_prob_decay\n",
    "\n",
    "    #new_c, new_h = array_ops.split(1, 2, new_state)\n",
    "\n",
    "    new_output = tf.cond(do_keep_looping, lambda: inputs, lambda: output)\n",
    "\n",
    "    return output, new_state, num_units, forget_bias, new_iteration_number, max_iterations, new_iteration_prob, iteration_prob_decay, new_iteration_activation, iteration_count, do_keep_looping\n",
    "\n",
    "def iterativeLSTM_LoopCondition(inputs, num_units, iteration_number, max_iterations,\n",
    "                                iteration_prob, iteration_prob_decay, iteration_activation, iteration_count, \n",
    "                                keep_looping):\n",
    "    return keep_looping\n",
    "\n",
    "\n",
    "def iterative_cell(inputs, num_units, iteration_activation, iteration_count, iteration_prob):\n",
    "    j_logits = linear([inputs], num_units, False, scope=\"j_logits\")\n",
    "    j_displacement = linear([iteration_count], num_units, True, scope=\"j_displacement\")\n",
    "    opposed_j = tanh( - j_logits + j_displacement)\n",
    "    new_info = tanh(tanh(j_logits + j_displacement) + tanh(opposed_j * sigmoid(opposed_j)))\n",
    "    logits = linear([new_info], num_units, True)\n",
    "\n",
    "    new_output = logits * iteration_activation + inputs * (1 - iteration_activation)\n",
    "\n",
    "    # In this approach the evidence of the iteration gate is based on the inputs that doesn't change over iterations and its state\n",
    "    p = linear([inputs, new_output], num_units, True, scope= \"iteration_activation\")\n",
    "\n",
    "\n",
    "    new_iteration_activation = update_iteration_activations(iteration_activation, floor(sigmoid(p) + iteration_prob))\n",
    "\n",
    "    return new_output, new_state, new_iteration_activation\n",
    "\n",
    "def update_iteration_activations(current_iteration_activations, new_iteration_activations):\n",
    "    # It is possible that other instances of the batch activate this cell, hence we need to avoid this\n",
    "    # by activate only those activations were this instance of the batch is actually activated\n",
    "    batch_iteration_activations = tf.reduce_max(current_iteration_activations, 1, True)\n",
    "    batch_iteration_activations_extended = tf.tile(batch_iteration_activations,\n",
    "        [1, int(current_iteration_activations.get_shape().dims[1].value\n",
    "            or new_iteration_activations.get_shape().dims[1].value)])\n",
    "\n",
    "    return new_iteration_activations * batch_iteration_activations_extended\n",
    "\n",
    "\n",
    "def variable_summaries(var, name, add_distribution=True, add_range=True, add_histogram=True):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        if add_distribution:\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.scalar_summary('mean/' + name, mean)\n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "                tf.scalar_summary('sttdev/' + name, stddev)\n",
    "\n",
    "        if add_range:\n",
    "            tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "            tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "\n",
    "        if add_histogram:\n",
    "            tf.histogram_summary(name, var)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate. Default will be 0.01 .')\n",
    "flags.DEFINE_integer('max_epochs', 1000, 'Number of epochs to run trainer.')\n",
    "flags.DEFINE_integer('hidden1', 4, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_integer('hidden2', 5, 'Number of units in hidden layer 2.')\n",
    "flags.DEFINE_string('train_dir', '/Users/leandro/Documents/Repositories/FIUBA/determination/data', \n",
    "                    'Directory to put the training data.')\n",
    "flags.DEFINE_string('log_dir', '/Users/leandro/Documents/Repositories/FIUBA/determination/logs', \n",
    "                    'Directory to put the log.')\n",
    "\n",
    "flags.DEFINE_boolean(\n",
    "    \"erase_logs_dir\", True,\n",
    "    \"An option to erase summaries in the logdir.\")\n",
    "\n",
    "FIRST_LAYER_HIDDEN_UNITS = flags.FLAGS.hidden1\n",
    "SECONG_LAYER_HIDDEN_UNITS = flags.FLAGS.hidden2\n",
    "LEARNING_RATE = flags.FLAGS.learning_rate\n",
    "MAX_EPOCHS = flags.FLAGS.max_epochs\n",
    "DATA_DIR = flags.FLAGS.train_dir\n",
    "LOG_DIR = flags.FLAGS.log_dir\n",
    "NUM_CLASSES = 2\n",
    "ERASE_LOGS_DIR = flags.FLAGS.erase_logs_dir\n",
    "\n",
    "\n",
    "def init_dir(dir_path):\n",
    "  if tf.gfile.Exists(dir_path) and ERASE_LOGS_DIR is True:\n",
    "    tf.gfile.DeleteRecursively(dir_path)\n",
    "  tf.gfile.MakeDirs(dir_path)\n",
    "\n",
    "init_dir(LOG_DIR)\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'X1':[0.0, 0.0, 1.0, 1.0], 'X2':[0.0, 1.0, 0.0, 1.0], 'X3':[0.0, 1.0, 1.0, 1.0], 'Y':[0.0, 1.0, 1.0, 2.0]})\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "df.to_csv(DATA_DIR + '/input.csv', index=None)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "data = pd.read_csv(DATA_DIR + '/input.csv')\n",
    "NUM_CLASSES = len(df.Y.unique())\n",
    "BATCH_SIZE = len(data)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "print 'Number of values to predict: {}'.format(NUM_CLASSES)\n",
    "print 'Number of examples to learn simultaneously: {}'.format(len(data))\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "inputs_data = data.ix[:, 0:len(data.columns)-1].as_matrix()\n",
    "if inputs_data is None:\n",
    "   inputs_data = [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "label_data = np.squeeze(data.ix[:, len(data.columns) - 1:len(data.columns)].as_matrix())\n",
    "if label_data is None:\n",
    "   label_data = [0.0, 1.0, 1.0, 0.0]\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "def dictionary_from_proto_buffer(proto_buffer):\n",
    "    summaries = {}\n",
    "    for val in proto_buffer.value:\n",
    "        # Assuming all summaries are scalars.\n",
    "        summaries[val.tag] = val.simple_value\n",
    "    return summaries\n",
    "\n",
    "def summarize_layer_determinations(layer_output):\n",
    "    number_of_components = layer_output.get_shape().dims[1].value\n",
    "    det_coefs = []\n",
    "    for i in range(number_of_components):  \n",
    "        unit_det_coeffs = []\n",
    "        for j in range(number_of_components):\n",
    "            corr_coef, update_op = tf.contrib.metrics.streaming_pearson_correlation(layer_output[:, i], \n",
    "                                                                                    layer_output[:, j])\n",
    "            det_coef = update_op**2\n",
    "            tf.summary.scalar('determination_' + str(i) + '_and_' + str(j), det_coef)\n",
    "            if j != i:\n",
    "                unit_det_coeffs.append(det_coef)\n",
    "        total_determination_of_unit = tf.reduce_sum(unit_det_coeffs)\n",
    "        det_coefs.append(total_determination_of_unit)\n",
    "        tf.summary.scalar('determination_of_' + str(i) + '_unit', total_determination_of_unit)\n",
    "    \n",
    "\n",
    "def build_model_layer(inputs, number_of_units, scope_name, use_relu, add_summaries=False):\n",
    "    with tf.name_scope(scope_name):\n",
    "        weights = tf.Variable(tf.truncated_normal([inputs.get_shape().dims[1].value, number_of_units], \n",
    "                                                 stddev=1.0 / math.sqrt(float(number_of_units))), name='weights')\n",
    "        \n",
    "        biases = tf.Variable(tf.zeros([number_of_units]), name='biases')\n",
    "        \n",
    "        logits = tf.matmul(inputs, weights) + biases\n",
    "        \n",
    "        layer_output = tf.nn.relu(logits) if use_relu is True else logits\n",
    "\n",
    "        if add_summaries is True:\n",
    "            summarize_layer_determinations(layer_output)\n",
    "\n",
    "    return layer_output\n",
    "        \n",
    "\n",
    "def build_model(inputs):\n",
    "    first_layer_output = build_model_layer(inputs=inputs, \n",
    "                                          number_of_units=FIRST_LAYER_HIDDEN_UNITS,\n",
    "                                          scope_name='first_hidden_layer',\n",
    "                                          use_relu=True,\n",
    "                                          add_summaries=True)\n",
    "    \n",
    "    second_layer_output = build_model_layer(inputs=first_layer_output, \n",
    "                                            number_of_units=SECONG_LAYER_HIDDEN_UNITS,\n",
    "                                            scope_name='second_hidden_layer',\n",
    "                                            use_relu=True,\n",
    "                                            add_summaries=True)\n",
    "    \n",
    "    output = build_model_layer(inputs=second_layer_output, \n",
    "                               number_of_units=NUM_CLASSES,\n",
    "                               scope_name='logits',\n",
    "                               use_relu=False,\n",
    "                               add_summaries=False)\n",
    "    return output\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    inputs = tf.placeholder(dtype=tf.float32, shape=[BATCH_SIZE, NUM_CLASSES])\n",
    "    label = tf.placeholder(dtype=tf.float32, shape=[BATCH_SIZE])\n",
    "    \n",
    "    model_output = build_model(inputs)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(model_output, tf.to_int64(label)), \n",
    "                          name='xentropy_mean')\n",
    "    \n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)\n",
    "    training = optimizer.minimize(loss, global_step=global_step)\n",
    "    \n",
    "    merge_summaries = tf.summary.merge_all()\n",
    "    summary_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)\n",
    "    \n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model_feed = {inputs:inputs_data, label:label_data}\n",
    "\n",
    "        _, loss_value, summaries = sess.run([training, loss, merge_summaries], feed_dict=model_feed)\n",
    "        if epoch // 100 == 0:\n",
    "            summary_writer.add_summary(summaries, global_step=epoch)\n",
    "            summary_writer.flush()\n",
    "            print 'epoch {}, loss = {}'.format(epoch, loss_value)\n",
    "    \n",
    "    summary_writer.close()\n",
    "    sess.close()\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "dictionary = dictionary_from_proto_buffer(tf.Summary.FromString(summaries))\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "def separate_layer_units_from_dictionary(dictionary):\n",
    "    layers = {}\n",
    "    for annotation in dictionary.keys():\n",
    "        if 'unit' in annotation:\n",
    "            layer_name, unit_name = annotation.rsplit('/', 1)\n",
    "            if layer_name in layers:\n",
    "                layers[layer_name].append(dict([('name', unit_name), ('value', dictionary[annotation])]))\n",
    "            else:\n",
    "                layers[layer_name] = [dict([('name', unit_name), ('value', dictionary[annotation])])]\n",
    "    return layers\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "layers_units_annotations = separate_layer_units_from_dictionary(dictionary)\n",
    "layers_units_annotations\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "def min_and_max_units_determinations_from_layer(layer_units_annotations):\n",
    "    max_determination_unit = {'name':'', 'value':0.0}\n",
    "    min_determination_unit = {'name':'', 'value':1.0}\n",
    "    for unit_annotation in layer_units_annotations:\n",
    "        if 'determination' in unit_annotation['name']:\n",
    "            if max_determination_unit['value'] < unit_annotation['value']:\n",
    "                max_determination_unit = unit_annotation\n",
    "            if min_determination_unit['value'] > unit_annotation['value']:\n",
    "                min_determination_unit = unit_annotation\n",
    "    return max_determination_unit, min_determination_unit\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "def layers_candidates_to_substraction_from_nn_dictionary(dictionary):\n",
    "    layers_candidates_to_substraction = {}\n",
    "    layer_candidate_to_reduction = {'name': None, 'value':0., 'cell':None}\n",
    "    for layer in dictionary:\n",
    "        max_determination_unit, min_determination_unit = min_and_max_units_determinations_from_layer(dictionary[layer])\n",
    "        layers_candidates_to_substraction[layer] = max_determination_unit\n",
    "        if layer_candidate_to_reduction['value'] < max_determination_unit['value']/len(dictionary[layer]):\n",
    "           layer_candidate_to_reduction = {'name':layer, \n",
    "                                           'value':max_determination_unit['value'], \n",
    "                                           'cell':max_determination_unit} \n",
    "    return layers_candidates_to_substraction, layer_candidate_to_reduction\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "layers_candidates_to_substraction, layer_candidate_to_reduction = layers_candidates_to_substraction_from_nn_dictionary(layers_units_annotations)\n",
    "\n",
    "\n",
    "# #### Cell of first layer with lowest Shapley value\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "layers_candidates_to_substraction['first_hidden_layer']['name']\n",
    "\n",
    "\n",
    "# #### Cell of second layer with lowest Shapley value\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "layers_candidates_to_substraction['second_hidden_layer']['name']\n",
    "\n",
    "\n",
    "# #### Layer with cell with lowest Shapley value\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "layer_candidate_to_reduction\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "show_graph(tf.get_default_graph().as_graph_def())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
